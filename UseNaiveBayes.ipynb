{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Make Classifier From Scratch\n",
        "최소한의 패키지를 가지고 다중 클래스분류기 만들기 프로젝트\n",
        "\n",
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 전처리기 만들기\n",
        "clean_text( ) 함수에 도큐먼트 묶음을 넣으면 전처리 한 후 다시 돌려보냄"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import names\n",
        "all_names = set(names.words())"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:23.421Z",
          "iopub.execute_input": "2020-05-24T10:30:23.426Z",
          "iopub.status.idle": "2020-05-24T10:30:23.432Z",
          "shell.execute_reply": "2020-05-24T10:30:23.437Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:24.360Z",
          "iopub.execute_input": "2020-05-24T10:30:24.365Z",
          "iopub.status.idle": "2020-05-24T10:30:24.375Z",
          "shell.execute_reply": "2020-05-24T10:30:24.380Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def letters_only(word):\n",
        "    return word.isalpha()"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:24.874Z",
          "iopub.execute_input": "2020-05-24T10:30:24.883Z",
          "iopub.status.idle": "2020-05-24T10:30:24.892Z",
          "shell.execute_reply": "2020-05-24T10:30:24.897Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(doc):\n",
        "    cleaned_doc = []\n",
        "    for word in doc.split(' '):\n",
        "        word = word.lower()\n",
        "  \n",
        "        if letters_only(word) and word not in all_names and len(word) > 2: # remove number and punc. and name entity\n",
        "            cleaned_doc.append(lemmatizer.lemmatize(word, \"v\"))\n",
        "    return ' '.join(cleaned_doc) "
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:25.382Z",
          "iopub.execute_input": "2020-05-24T10:30:25.388Z",
          "iopub.status.idle": "2020-05-24T10:30:25.396Z",
          "shell.execute_reply": "2020-05-24T10:30:25.402Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorizer를 만들어준다.\n",
        "나이브 베이즈가 성능이 제일 좋아서 이걸 제출하지만, SVM도 해봤기 때문에 tfidf와 CountVectorizer도 필요했다. \n",
        "Vectorizer에 파라미터로 tfidf를 True혹은 False로 해서 tfidf를 해줄지 말지를 결정할 수 있도록 했다. 그 외에도 max_len으로 최대 몇개의 단어를 사용할지, stop_word를 뺄지 말지의 옵션을 두었다. bigram과 max_df옵션도 주었는데, bigram의 경우 개수를 넣는 것이 아닌 True, False값을 가지도록 설계하였다. max_df의 경우 최대 몇개 이상의 것은 무시하는 것인데, 기본값은 무한으로 제한이 없다."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "class Vectorizer:\n",
        "    def __init__(self, max_len = 100000, tfidf = False, stop_word = False, max_df = float(\"inf\"),\n",
        "                 min_df = -float(\"inf\"), bigram = False):\n",
        "        self.max_len = max_len\n",
        "        self.tfidf = tfidf\n",
        "        self.stop_word = stop_word\n",
        "        self.max_df = max_df\n",
        "        self.ngram = bigram\n",
        "        self.min_df = min_df\n",
        "    \n",
        "    def vocab_(self):\n",
        "        return self.word_count\n",
        "    \n",
        "    def n_gram(self, text, n):\n",
        "        fi = text.split(\" \")\n",
        "        ret_list = [None for i in range(len(fi) -n +1)]\n",
        "        for i in range(len(fi) -n +1):\n",
        "            add = \"\"\n",
        "            for k in range(n-1):\n",
        "                add += fi[i + k]\n",
        "                add += \" \"\n",
        "            add += fi[i+n-1]\n",
        "            ret_list[i] = add\n",
        "        return ret_list\n",
        "    \n",
        "    \n",
        "    def make_vector(self, word_count):\n",
        "        self.word_count = word_count\n",
        "        sort_by_count = sorted(word_count.items(), key = lambda x:x[1][0], reverse = True)\n",
        "        how_many = len(sort_by_count)\n",
        "        if self.max_len > how_many:\n",
        "            how_many = how_many\n",
        "        elif self.max_len <= how_many:\n",
        "            how_many = self.max_len\n",
        "            \n",
        "        count = 0\n",
        "        index_of_words = {}\n",
        "        self.idfs = []\n",
        "        for key, value in sort_by_count:\n",
        "            if count >= how_many:\n",
        "                break\n",
        "            if self.stop_word == True:\n",
        "                if key not in stopwords.words(\"english\"):\n",
        "                    index_of_words[key] = count\n",
        "                    if self.tfidf == True:\n",
        "                        self.idfs.append(value[2])\n",
        "                    count += 1\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                index_of_words[key] = count\n",
        "                if self.tfidf == True:\n",
        "                    self.idfs.append(value[2])\n",
        "                count += 1\n",
        "\n",
        "        self.max_len = how_many\n",
        "        self.index_of_words = index_of_words\n",
        "                \n",
        "        \n",
        "    def tfidfVect(self, word_count):\n",
        "        nD = len(self.texts)\n",
        "        for key, value in sorted(word_count.items(), key = lambda x:x[1][0], reverse = True):\n",
        "            idf = math.log(nD/(1+value[1]))\n",
        "            word_count[key].append(idf)\n",
        "            word_count[key][0] = value[0] * idf\n",
        "        \n",
        "        self.make_vector(word_count)\n",
        "        \n",
        "    def countVect(self, word_count):\n",
        "        self.make_vector(word_count)\n",
        "        \n",
        "    def fit(self, texts):\n",
        "        self.texts = texts\n",
        "        word_count = {}\n",
        "            \n",
        "        #word_count = {단어: [전체단어개수,단어가 나온 문장개수]}\n",
        "        for text in texts:\n",
        "            word_in_text = []\n",
        "            ts = text.split()\n",
        "            if self.ngram != False:\n",
        "                ts += self.n_gram(text, 2)\n",
        "            for word in ts:\n",
        "                if word not in word_count.keys():\n",
        "                    word_count[word] = [1, 1]\n",
        "                    word_in_text.append(word)\n",
        "                else:\n",
        "                    word_count[word][0] += 1\n",
        "                    word_in_text.append(word)\n",
        "            \n",
        "            for i in set(word_in_text):\n",
        "                word_count[i][1] += 1\n",
        "        \n",
        "        chaier = []\n",
        "        for key, value in word_count.items():\n",
        "            if value[0] > self.max_df and value[0] < self.min_df:\n",
        "                chaier.append(key)\n",
        "        for i in chaier:\n",
        "            del word_count[i]\n",
        "        \n",
        "        if self.tfidf:\n",
        "            self.tfidfVect(word_count)\n",
        "        \n",
        "        else:\n",
        "            self.countVect(word_count)\n",
        "        \n",
        "    def transform(self, texts):\n",
        "        ret_list = [[0 for i in range(self.max_len)] for i in range(len(texts))]\n",
        "        for i in range(len(texts)):\n",
        "            text = texts[i]\n",
        "            ts = text.split()\n",
        "            if self.ngram != False:\n",
        "                text = self.n_gram(text, 2)\n",
        "                ts = text\n",
        "            for word in ts:\n",
        "#                 print(word)\n",
        "                if word in self.index_of_words.keys():\n",
        "#                     print(self.index_of_words[word])\n",
        "                    ret_list[i][self.index_of_words[word]] += 1\n",
        "        \n",
        "        ret_list = np.array(ret_list)\n",
        "        idfs = np.array(self.idfs)\n",
        "        print(ret_list.shape, idfs.shape)\n",
        "        if self.tfidf == True:\n",
        "            ret_list = ret_list * idfs\n",
        "        \n",
        "        return ret_list"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:26.291Z",
          "iopub.execute_input": "2020-05-24T10:30:26.297Z",
          "iopub.status.idle": "2020-05-24T10:30:26.305Z",
          "shell.execute_reply": "2020-05-24T10:30:26.314Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NaiveBayes Classifier를 만들어준다.\n",
        "class로 만들어서 여러번의 함수를 호출하지 않고도 동일한 작업을 수행할 수 있도록 한다.\n",
        "smoothing옵션을 클래스의 초기 파라미터로 줄 수 있다. 기본값은 0이다. 그러므로 0을 방지하려면 1로 스무딩 값을 주어야 한다."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class NaiveBayes:\n",
        "    def __init__(self, smoothing = 0):\n",
        "        self.smoothing = smoothing\n",
        "        self.term_docs_train_dict = {}\n",
        "        self.y_ = {}\n",
        "        \n",
        "    def _split_sort(self, X, y):\n",
        "#         print(X.shape)\n",
        "        for label in range(self.how_many_label):\n",
        "            y_temp = []\n",
        "            x_temp = []\n",
        "            for i in range(len(y)):\n",
        "#                 print(i)\n",
        "                if y[i] == label:\n",
        "                    y_temp.append(1)\n",
        "                    x_temp.append(X[i])\n",
        "                else:\n",
        "                    y_temp.append(-1)\n",
        "                    x_temp.append(X[i])\n",
        "            self.term_docs_train_dict[label] = np.array(x_temp)\n",
        "            self.y_[label] = y_temp\n",
        "    \n",
        "    def _get_label_index(self, labels):\n",
        "        label_index = defaultdict(list)\n",
        "        for index, label in enumerate(labels):\n",
        "            label_index[label].append(index)\n",
        "        return label_index\n",
        "    \n",
        "    def _get_prior(self, label_index):\n",
        "        prior = {label: len(index) for label, index in label_index.items()}\n",
        "        pr_zero = prior[1]\n",
        "        pr_one = prior[-1]\n",
        "\n",
        "        prior[1] = pr_zero/(pr_zero+ pr_one)\n",
        "        prior[-1] = pr_one/(pr_zero+pr_one)\n",
        "\n",
        "        return prior\n",
        "    \n",
        "    def _get_likelihood(self, term_doc_matrix, label_index, smoothing=0):\n",
        "        likelihood = {}\n",
        "        word_count = term_doc_matrix.shape[1]\n",
        "        total_word_len_in_s = {}\n",
        "        total_word_set_in_s = {}\n",
        "        for label, index in label_index.items():\n",
        "            whole = 0\n",
        "            for ix in index:\n",
        "                whole += sum(term_doc_matrix[ix])\n",
        "            total_word_len_in_s[label] = whole\n",
        "        for label, index in label_index.items():\n",
        "            whole = 0\n",
        "            for ix in index:\n",
        "                count = 0\n",
        "                for tmp in term_doc_matrix[ix]:\n",
        "                    if tmp != 0:\n",
        "                        count += 1\n",
        "                whole += count\n",
        "            total_word_set_in_s[label] = whole\n",
        "        likelihood[1] = np.zeros(word_count)\n",
        "        likelihood[-1] = np.zeros(word_count)\n",
        "        lab_one = np.array([term_doc_matrix[idx] for idx in label_index[-1]])\n",
        "        lab_zero = np.array([term_doc_matrix[idx] for idx in label_index[1]])\n",
        "        sum_one = lab_one.sum(axis = 0)\n",
        "        sum_zero = lab_zero.sum(axis = 0)\n",
        "        for i in range(word_count):\n",
        "            likelihood[1][i] = (sum_zero[i]+smoothing) / (total_word_len_in_s[1]+total_word_set_in_s[1])\n",
        "            likelihood[-1][i] = (sum_one[i]+smoothing) / (total_word_len_in_s[-1]+total_word_set_in_s[-1])\n",
        "\n",
        "        return likelihood\n",
        "    \n",
        "    \n",
        "    def _get_posterior(self, term_doc_matrix, prior, likelihood):\n",
        "\n",
        "        a = []\n",
        "        tt = term_doc_matrix.sum(axis=1)\n",
        "        for i in range(term_doc_matrix.shape[0]):\n",
        "            a.append((term_doc_matrix[i]/tt[i])*100)\n",
        "        term_doc_matrix = np.array(a)\n",
        "\n",
        "\n",
        "        num_docs = term_doc_matrix.shape[0]\n",
        "        posteriors = [None for i in range(num_docs)]\n",
        "        posterior_yes = likelihood[1]\n",
        "        posterior_no = likelihood[-1]\n",
        "\n",
        "\n",
        "        posterior_yes = (np.log(posterior_yes) * term_doc_matrix).sum(axis = 1) + np.log(prior[1])\n",
        "        posterior_no = (np.log(posterior_no) * term_doc_matrix).sum(axis = 1) + np.log(prior[-1])\n",
        "        posterior_yes = np.exp(posterior_yes)\n",
        "        posterior_no = np.exp(posterior_no)\n",
        "        num_docs = term_doc_matrix.shape[0]\n",
        "\n",
        "        for i in range(num_docs):\n",
        "            if posterior_yes[i] == 0 and posterior_no[i] == 0:\n",
        "                if posterior_yes[i] < posterior_no[i]:        \n",
        "                    tmp = {-1: 0, 1:1}\n",
        "                else:\n",
        "                    tmp = {-1:1,1:0}\n",
        "                posteriors[i] =tmp\n",
        "                continue\n",
        "\n",
        "            tmp = {-1: posterior_no[i]/(posterior_yes[i]+posterior_no[i]), 1:posterior_yes[i]/(posterior_yes[i]+posterior_no[i])}\n",
        "\n",
        "            posteriors[i] =tmp\n",
        "\n",
        "        return posteriors\n",
        "\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        self.how_many_label = len(set(y))\n",
        "        self._split_sort(X, y)\n",
        "        self.label_index_dict = {}\n",
        "        for lab in range(self.how_many_label):\n",
        "            label_index = self._get_label_index(self.y_[lab])\n",
        "            self.label_index_dict[lab] = label_index\n",
        "\n",
        "        self.prior_dict = {}\n",
        "        for lab in range(self.how_many_label):\n",
        "            prior = self._get_prior(self.label_index_dict[lab])\n",
        "            self.prior_dict[lab] = prior\n",
        "        \n",
        "        self.likelihood_dict = {}\n",
        "        for lab in range(self.how_many_label):\n",
        "            likelihood = self._get_likelihood(self.term_docs_train_dict[lab], self.label_index_dict[lab], self.smoothing) \n",
        "            self.likelihood_dict[lab] = likelihood\n",
        "        \n",
        "    def predict(self, X):\n",
        "        posteriors_dict = {}\n",
        "        for lab in range(self.how_many_label):\n",
        "            posteriors = self._get_posterior(X, self.prior_dict[lab], self.likelihood_dict[lab])\n",
        "            posteriors_dict[lab] = posteriors\n",
        "            \n",
        "        a = {}\n",
        "        for text in range(X.shape[0]):\n",
        "            max_id = -1\n",
        "            max_val = -float(\"inf\")\n",
        "            for lab in range(5):    \n",
        "                if max_val < posteriors_dict[lab][text][1]:\n",
        "                    max_id = lab\n",
        "                    max_val = posteriors_dict[lab][text][1]\n",
        "            a[text] = max_id\n",
        "        return np.array(list(a.values()))"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:26.931Z",
          "iopub.execute_input": "2020-05-24T10:30:26.936Z",
          "iopub.status.idle": "2020-05-24T10:30:26.944Z",
          "shell.execute_reply": "2020-05-24T10:30:26.955Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 정확도 측정 함수\n",
        "a,b의 값을 비교해 같은 것의 개수를 센다. 그 후 전체 개수로 나눈다."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_function(a, b):\n",
        "    b = list(b)\n",
        "    a = list(a)\n",
        "    count = 0\n",
        "    for i in range(len(b)):\n",
        "        if a[i] == b[i]:\n",
        "            count+= 1\n",
        "    return count / len(b)"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:27.458Z",
          "iopub.execute_input": "2020-05-24T10:30:27.462Z",
          "iopub.status.idle": "2020-05-24T10:30:27.470Z",
          "shell.execute_reply": "2020-05-24T10:30:27.475Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pandas를 통해 train과 test에 해당하는 csv파일을 불러와 저장한다."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"TRAIN.csv\")\n",
        "test_df = pd.read_csv(\"TEST.csv\")\n",
        "x_train = train_df.iloc[:, 2]\n",
        "y_train = train_df.iloc[:,-1].astype(int)\n",
        "x_train = [clean_text(doc) for doc in x_train]\n",
        "x_test = test_df[\"Post\"]\n",
        "x_test = [clean_text(doc) for doc in x_test]\n",
        "print(len(x_train), len(x_test))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "350 150\n"
          ]
        }
      ],
      "execution_count": 35,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:28.098Z",
          "iopub.execute_input": "2020-05-24T10:30:28.105Z",
          "iopub.status.idle": "2020-05-24T10:30:29.708Z",
          "shell.execute_reply": "2020-05-24T10:30:29.772Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train data분리\n",
        "test데이터는 label이 없기 때문에 정확성 테스트를 위해 train data를 분할해준다. 분할해주는 함수를 직접 만들어 사용하였다."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_spliter(X, y, test_rate = 0.858):\n",
        "    standard = int(len(X)* test_rate)\n",
        "    return (X[:standard], X[standard:]), (y[:standard], y[standard:])"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:29.715Z",
          "iopub.execute_input": "2020-05-24T10:30:29.722Z",
          "iopub.status.idle": "2020-05-24T10:30:29.730Z",
          "shell.execute_reply": "2020-05-24T10:30:29.775Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x_train,test_x_train), (train_y_train, test_y_train) = train_test_spliter(x_train, y_train)"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:29.737Z",
          "iopub.execute_input": "2020-05-24T10:30:29.742Z",
          "iopub.status.idle": "2020-05-24T10:30:29.749Z",
          "shell.execute_reply": "2020-05-24T10:30:29.780Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 라벨이 있는 train data를 분리한 것을 통해 정확도를 테스트한다.\n",
        "제작한 Vectorizer, NaiveBayes를 활용하여 분석한다."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cv = Vectorizer(1150, tfidf = True, stop_word = False, max_df=500, min_df = 5, bigram = True)\n",
        "cv.fit(x_train)\n",
        "term_docs_train = cv.transform(train_x_train)\n",
        "term_docs_test = cv.transform(test_x_train)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300, 1150) (1150,)\n",
            "(50, 1150) (1150,)\n"
          ]
        }
      ],
      "execution_count": 38,
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:29.758Z",
          "iopub.execute_input": "2020-05-24T10:30:29.800Z",
          "iopub.status.idle": "2020-05-24T10:30:30.897Z",
          "shell.execute_reply": "2020-05-24T10:30:30.922Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NB = NaiveBayes(smoothing = 0.1)\n",
        "NB.fit(term_docs_train, train_y_train)\n",
        "y_pred = NB.predict(term_docs_test)"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:30.904Z",
          "iopub.execute_input": "2020-05-24T10:30:30.908Z",
          "iopub.status.idle": "2020-05-24T10:30:31.598Z",
          "shell.execute_reply": "2020-05-24T10:30:31.620Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_function(y_pred, test_y_train)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 40,
          "data": {
            "text/plain": [
              "0.32"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 40,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:31.604Z",
          "iopub.execute_input": "2020-05-24T10:30:31.608Z",
          "iopub.status.idle": "2020-05-24T10:30:31.617Z",
          "shell.execute_reply": "2020-05-24T10:30:31.622Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 라벨이 없는 test data를 예측하여 csv파일로 바꾸어준다.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cv = Vectorizer(1150, tfidf = True, stop_word = False, max_df=500, min_df = 5, bigram = True)\n",
        "cv.fit(x_train)\n",
        "term_docs_train = cv.transform(x_train)\n",
        "test_term_docs = cv.transform(x_test)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(350, 1150) (1150,)\n",
            "(150, 1150) (1150,)\n"
          ]
        }
      ],
      "execution_count": 41,
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:34.041Z",
          "iopub.execute_input": "2020-05-24T10:30:34.080Z",
          "iopub.status.idle": "2020-05-24T10:30:35.336Z",
          "shell.execute_reply": "2020-05-24T10:30:35.346Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NB = NaiveBayes(smoothing = 0.1)\n",
        "NB.fit(term_docs_train, y_train)\n",
        "test_pred = NB.predict(test_term_docs)"
      ],
      "outputs": [],
      "execution_count": 42,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:36.045Z",
          "iopub.execute_input": "2020-05-24T10:30:36.049Z",
          "iopub.status.idle": "2020-05-24T10:30:36.883Z",
          "shell.execute_reply": "2020-05-24T10:30:36.886Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 43,
          "data": {
            "text/plain": [
              "array([4, 3, 3, 1, 3, 2, 4, 1, 4, 1, 0, 0, 0, 3, 0, 4, 4, 1, 4, 2, 2, 4,\n",
              "       0, 4, 2, 4, 4, 3, 4, 1, 1, 1, 1, 2, 0, 4, 4, 1, 3, 4, 4, 3, 3, 1,\n",
              "       4, 3, 3, 4, 0, 3, 4, 3, 4, 3, 3, 3, 0, 1, 3, 0, 3, 4, 2, 2, 3, 4,\n",
              "       1, 2, 3, 4, 0, 1, 4, 4, 3, 4, 4, 4, 2, 4, 2, 1, 4, 2, 2, 0, 4, 4,\n",
              "       3, 0, 3, 3, 2, 4, 2, 4, 4, 4, 2, 1, 4, 1, 2, 2, 1, 4, 4, 1, 1, 4,\n",
              "       3, 2, 2, 2, 4, 0, 4, 2, 4, 0, 0, 1, 3, 0, 4, 2, 4, 2, 2, 2, 4, 4,\n",
              "       3, 1, 4, 3, 0, 1, 1, 2, 0, 4, 1, 4, 4, 4, 1, 4, 1, 3])"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 43,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:37.594Z",
          "iopub.execute_input": "2020-05-24T10:30:37.598Z",
          "iopub.status.idle": "2020-05-24T10:30:37.612Z",
          "shell.execute_reply": "2020-05-24T10:30:37.617Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(test_pred).to_csv(\"2018310412__.csv\", index=False)"
      ],
      "outputs": [],
      "execution_count": 44,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-05-24T10:30:38.738Z",
          "iopub.execute_input": "2020-05-24T10:30:38.743Z",
          "iopub.status.idle": "2020-05-24T10:30:38.751Z",
          "shell.execute_reply": "2020-05-24T10:30:38.755Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.7",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.23.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

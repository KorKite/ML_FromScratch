{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Classifier From Scratch\n",
    "최소한의 패키지를 가지고 다중 클래스분류기 만들기 프로젝트\n",
    "\n",
    "2018310412 인공지능융합전공 고준서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리기 만들기\n",
    "clean_text( ) 함수에 도큐먼트 묶음을 넣으면 전처리 한 후 다시 돌려보냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "all_names = set(names.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letters_only(word):\n",
    "    return word.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(doc):\n",
    "    cleaned_doc = []\n",
    "    for word in doc.split(' '):\n",
    "        word = word.lower()\n",
    "  \n",
    "        if letters_only(word) and word not in all_names and len(word) > 2: # remove number and punc. and name entity\n",
    "            cleaned_doc.append(lemmatizer.lemmatize(word, \"v\"))\n",
    "    return ' '.join(cleaned_doc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer를 만들어준다.\n",
    "나이브 베이즈가 성능이 제일 좋아서 이걸 제출하지만, SVM도 해봤기 때문에 tfidf와 CountVectorizer도 필요했다. \n",
    "Vectorizer에 파라미터로 tfidf를 True혹은 False로 해서 tfidf를 해줄지 말지를 결정할 수 있도록 했다. 그 외에도 성능을 체크하기 위해 max_len, stop_word같은 옵션을 두었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "class Vectorizer:\n",
    "    def __init__(self, max_len = 100000, tfidf = False, stop_word = False, max_df = float(\"inf\")):\n",
    "        self.max_len = max_len\n",
    "        self.tfidf = tfidf\n",
    "        self.stop_word = stop_word\n",
    "        self.max_df = max_df\n",
    "    \n",
    "    def vocab_(self):\n",
    "        return self.word_count\n",
    "    \n",
    "    def make_vector(self, word_count):\n",
    "        self.word_count = word_count\n",
    "        sort_by_count = sorted(word_count.items(), key = lambda x:x[1][0], reverse = True)\n",
    "        how_many = len(sort_by_count)\n",
    "        if self.max_len > how_many:\n",
    "            how_many = how_many\n",
    "        elif self.max_len <= how_many:\n",
    "            how_many = self.max_len\n",
    "            \n",
    "        count = 0\n",
    "        index_of_words = {}\n",
    "        self.idfs = []\n",
    "        for key, value in sort_by_count:\n",
    "            if count >= how_many:\n",
    "                break\n",
    "            if self.stop_word == True:\n",
    "                if key not in stopwords.words(\"english\"):\n",
    "                    index_of_words[key] = count\n",
    "                    if self.tfidf == True:\n",
    "                        self.idfs.append(value[2])\n",
    "                    count += 1\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                index_of_words[key] = count\n",
    "                if self.tfidf == True:\n",
    "                    self.idfs.append(value[2])\n",
    "                count += 1\n",
    "\n",
    "        self.max_len = how_many\n",
    "        self.index_of_words = index_of_words\n",
    "                \n",
    "        \n",
    "    def tfidfVect(self, word_count):\n",
    "        nD = len(self.texts)\n",
    "        for key, value in sorted(word_count.items(), key = lambda x:x[1][0], reverse = True):\n",
    "            idf = math.log(nD/(1+value[1]))\n",
    "            word_count[key].append(idf)\n",
    "            word_count[key][0] = value[0] * idf\n",
    "        \n",
    "        self.make_vector(word_count)\n",
    "        \n",
    "    def countVect(self, word_count):\n",
    "        self.make_vector(word_count)\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        self.texts = texts\n",
    "        word_count = {}\n",
    "        #word_count = {단어: [전체단어개수,단어가 나온 문장개수]}\n",
    "        for text in texts:\n",
    "            word_in_text = []\n",
    "            \n",
    "            for word in text.split():\n",
    "                if word not in word_count.keys():\n",
    "                    word_count[word] = [1, 1]\n",
    "                    word_in_text.append(word)\n",
    "                else:\n",
    "                    word_count[word][0] += 1\n",
    "                    word_in_text.append(word)\n",
    "            \n",
    "            for i in set(word_in_text):\n",
    "                word_count[i][1] += 1\n",
    "        \n",
    "        chaier = []\n",
    "        for key, value in word_count.items():\n",
    "            if value[0] > self.max_df:\n",
    "                chaier.append(key)\n",
    "        for i in chaier:\n",
    "            del word_count[i]\n",
    "        \n",
    "        if self.tfidf:\n",
    "            self.tfidfVect(word_count)\n",
    "        \n",
    "        else:\n",
    "            self.countVect(word_count)\n",
    "        \n",
    "    def transform(self, texts):\n",
    "        ret_list = [[0 for i in range(self.max_len)] for i in range(len(texts))]\n",
    "        for i in range(len(texts)):\n",
    "            text = texts[i]\n",
    "            for word in text.split():\n",
    "#                 print(word)\n",
    "                if word in self.index_of_words.keys():\n",
    "#                     print(self.index_of_words[word])\n",
    "                    ret_list[i][self.index_of_words[word]] += 1\n",
    "        \n",
    "        ret_list = np.array(ret_list)\n",
    "        idfs = np.array(self.idfs)\n",
    "        print(ret_list.shape, idfs.shape)\n",
    "        if self.tfidf == True:\n",
    "            ret_list = ret_list * idfs\n",
    "        \n",
    "        return ret_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaiveBayes Classifier를 만들어준다.\n",
    "class로 만들어서 여러번의 함수를 호출하지 않고도 동일한 작업을 수행할 수 있도록 한다.\n",
    "smoothing옵션을 파라미터로 줄 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, smoothing = 0):\n",
    "        self.smoothing = smoothing\n",
    "        self.term_docs_train_dict = {}\n",
    "        self.y_ = {}\n",
    "        \n",
    "    def _split_sort(self, X, y):\n",
    "#         print(X.shape)\n",
    "        for label in range(self.how_many_label):\n",
    "            y_temp = []\n",
    "            x_temp = []\n",
    "            for i in range(len(y)):\n",
    "#                 print(i)\n",
    "                if y[i] == label:\n",
    "                    y_temp.append(1)\n",
    "                    x_temp.append(X[i])\n",
    "                else:\n",
    "                    y_temp.append(-1)\n",
    "                    x_temp.append(X[i])\n",
    "            self.term_docs_train_dict[label] = np.array(x_temp)\n",
    "            self.y_[label] = y_temp\n",
    "    \n",
    "    def _get_label_index(self, labels):\n",
    "        label_index = defaultdict(list)\n",
    "        for index, label in enumerate(labels):\n",
    "            label_index[label].append(index)\n",
    "        return label_index\n",
    "    \n",
    "    def _get_prior(self, label_index):\n",
    "        prior = {label: len(index) for label, index in label_index.items()}\n",
    "        pr_zero = prior[1]\n",
    "        pr_one = prior[-1]\n",
    "\n",
    "        prior[1] = pr_zero/(pr_zero+ pr_one)\n",
    "        prior[-1] = pr_one/(pr_zero+pr_one)\n",
    "\n",
    "        return prior\n",
    "    \n",
    "    def _get_likelihood(self, term_doc_matrix, label_index, smoothing=0):\n",
    "        likelihood = {}\n",
    "        word_count = term_doc_matrix.shape[1]\n",
    "        total_word_len_in_s = {}\n",
    "        total_word_set_in_s = {}\n",
    "        for label, index in label_index.items():\n",
    "            whole = 0\n",
    "            for ix in index:\n",
    "                whole += sum(term_doc_matrix[ix])\n",
    "            total_word_len_in_s[label] = whole\n",
    "        for label, index in label_index.items():\n",
    "            whole = 0\n",
    "            for ix in index:\n",
    "                count = 0\n",
    "                for tmp in term_doc_matrix[ix]:\n",
    "                    if tmp != 0:\n",
    "                        count += 1\n",
    "                whole += count\n",
    "            total_word_set_in_s[label] = whole\n",
    "        likelihood[1] = np.zeros(word_count)\n",
    "        likelihood[-1] = np.zeros(word_count)\n",
    "        lab_one = np.array([term_doc_matrix[idx] for idx in label_index[-1]])\n",
    "        lab_zero = np.array([term_doc_matrix[idx] for idx in label_index[1]])\n",
    "        sum_one = lab_one.sum(axis = 0)\n",
    "        sum_zero = lab_zero.sum(axis = 0)\n",
    "        for i in range(word_count):\n",
    "            likelihood[1][i] = (sum_zero[i]+smoothing) / (total_word_len_in_s[1]+total_word_set_in_s[1])\n",
    "            likelihood[-1][i] = (sum_one[i]+smoothing) / (total_word_len_in_s[-1]+total_word_set_in_s[-1])\n",
    "\n",
    "        return likelihood\n",
    "    \n",
    "    \n",
    "    def _get_posterior(self, term_doc_matrix, prior, likelihood):\n",
    "\n",
    "        a = []\n",
    "        tt = term_doc_matrix.sum(axis=1)\n",
    "        for i in range(term_doc_matrix.shape[0]):\n",
    "            a.append((term_doc_matrix[i]/tt[i])*100)\n",
    "        term_doc_matrix = np.array(a)\n",
    "\n",
    "\n",
    "        num_docs = term_doc_matrix.shape[0]\n",
    "        posteriors = [None for i in range(num_docs)]\n",
    "        posterior_yes = likelihood[1]\n",
    "        posterior_no = likelihood[-1]\n",
    "\n",
    "\n",
    "        posterior_yes = (np.log(posterior_yes) * term_doc_matrix).sum(axis = 1) + np.log(prior[1])\n",
    "        posterior_no = (np.log(posterior_no) * term_doc_matrix).sum(axis = 1) + np.log(prior[-1])\n",
    "        posterior_yes = np.exp(posterior_yes)\n",
    "        posterior_no = np.exp(posterior_no)\n",
    "        num_docs = term_doc_matrix.shape[0]\n",
    "\n",
    "        for i in range(num_docs):\n",
    "            if posterior_yes[i] == 0 and posterior_no[i] == 0:\n",
    "                if posterior_yes[i] < posterior_no[i]:        \n",
    "                    tmp = {-1: 0, 1:1}\n",
    "                else:\n",
    "                    tmp = {-1:1,1:0}\n",
    "                posteriors[i] =tmp\n",
    "                continue\n",
    "\n",
    "            tmp = {-1: posterior_no[i]/(posterior_yes[i]+posterior_no[i]), 1:posterior_yes[i]/(posterior_yes[i]+posterior_no[i])}\n",
    "\n",
    "            posteriors[i] =tmp\n",
    "\n",
    "        return posteriors\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.how_many_label = len(set(y))\n",
    "        self._split_sort(X, y)\n",
    "        self.label_index_dict = {}\n",
    "        for lab in range(self.how_many_label):\n",
    "            label_index = self._get_label_index(self.y_[lab])\n",
    "            self.label_index_dict[lab] = label_index\n",
    "\n",
    "        self.prior_dict = {}\n",
    "        for lab in range(self.how_many_label):\n",
    "            prior = self._get_prior(self.label_index_dict[lab])\n",
    "            self.prior_dict[lab] = prior\n",
    "        \n",
    "        self.likelihood_dict = {}\n",
    "        for lab in range(self.how_many_label):\n",
    "            likelihood = self._get_likelihood(self.term_docs_train_dict[lab], self.label_index_dict[lab], self.smoothing) \n",
    "            self.likelihood_dict[lab] = likelihood\n",
    "        \n",
    "    def predict(self, X):\n",
    "        posteriors_dict = {}\n",
    "        for lab in range(self.how_many_label):\n",
    "            posteriors = self._get_posterior(X, self.prior_dict[lab], self.likelihood_dict[lab])\n",
    "            posteriors_dict[lab] = posteriors\n",
    "            \n",
    "        a = {}\n",
    "        for text in range(X.shape[0]):\n",
    "            max_id = -1\n",
    "            max_val = -float(\"inf\")\n",
    "            for lab in range(5):    \n",
    "                if max_val < posteriors_dict[lab][text][1]:\n",
    "                    max_id = lab\n",
    "                    max_val = posteriors_dict[lab][text][1]\n",
    "            a[text] = max_id\n",
    "        return list(a.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정확도 측정 함수\n",
    "a,b의 값을 비교해 같으면 count를 올리고 전체 개수로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(a, b):\n",
    "    b = list(b)\n",
    "    a = list(a)\n",
    "    count = 0\n",
    "    for i in range(len(b)):\n",
    "        if a[i] == b[i]:\n",
    "            count+= 1\n",
    "    return count / len(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas를 통해 train과 test에 해당하는 csv파일을 불러와 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 150\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"TRAIN.csv\")\n",
    "test_df = pd.read_csv(\"TEST.csv\")\n",
    "x_train = train_df.iloc[:, 2]\n",
    "y_train = train_df.iloc[:,-1].astype(int)\n",
    "x_train = [clean_text(doc) for doc in x_train]\n",
    "x_test = test_df[\"Post\"]\n",
    "x_test = [clean_text(doc) for doc in x_test]\n",
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train data분리\n",
    "test데이터는 label이 없기 때문에 정확성 테스트를 위해 train data를 분할해준다. 분할해주는 함수를 직접 만들어 사용하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_spliter(X, y, test_rate = 0.858):\n",
    "    standard = int(len(X)* test_rate)\n",
    "    return (X[:standard], X[standard:]), (y[:standard], y[standard:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x_train,test_x_train), (train_y_train, test_y_train) = train_test_spliter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라벨이 있는 train data를 분리한 것을 통해 정확도를 테스트한다.\n",
    "제작한 Vectorizer, NaiveBayes를 활용하여 분석한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 600) (0,)\n",
      "(50, 600) (0,)\n"
     ]
    }
   ],
   "source": [
    "cv = Vectorizer(max_len = 600, tfidf = False, stop_word = True, max_df = 500)\n",
    "cv.fit(x_train)\n",
    "term_docs_train = cv.transform(train_x_train)\n",
    "term_docs_test = cv.transform(test_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = NaiveBayes(smoothing = 1)\n",
    "NB.fit(term_docs_train, train_y_train)\n",
    "y_pred = NB.predict(term_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_function(y_pred, test_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dbt': [5, 3],\n",
       " 'mind': [265, 122],\n",
       " 'explain': [87, 60],\n",
       " 'little': [317, 140],\n",
       " 'bite': [240, 114],\n",
       " 'sorry': [358, 117],\n",
       " 'hear': [306, 120],\n",
       " 'suppose': [66, 51],\n",
       " 'actually': [313, 151],\n",
       " 'mean': [357, 139],\n",
       " 'definitely': [110, 63],\n",
       " 'person': [444, 153],\n",
       " 'honestly': [78, 59],\n",
       " 'sure': [404, 154],\n",
       " 'why': [488, 169],\n",
       " 'shun': [2, 3],\n",
       " 'sound': [466, 148],\n",
       " 'anything': [469, 166],\n",
       " 'plan': [138, 76],\n",
       " 'pain': [307, 129],\n",
       " 'ish': [1, 2],\n",
       " 'cut': [127, 70],\n",
       " 'each': [132, 84],\n",
       " 'tire': [336, 98],\n",
       " 'concern': [63, 42],\n",
       " 'symptoms': [26, 19],\n",
       " 'wake': [84, 59],\n",
       " 'bad': [376, 150],\n",
       " 'dream': [62, 44],\n",
       " 'hit': [91, 66],\n",
       " 'rock': [34, 25],\n",
       " 'bottom': [12, 12],\n",
       " 'pleasant': [7, 8],\n",
       " 'lady': [4, 5],\n",
       " 'conduct': [3, 4],\n",
       " 'therapy': [87, 56],\n",
       " 'sessions': [1, 2],\n",
       " 'kill': [479, 202],\n",
       " 'especially': [112, 66],\n",
       " 'since': [220, 127],\n",
       " 'youve': [384, 121],\n",
       " 'carry': [44, 36],\n",
       " 'chance': [136, 83],\n",
       " 'bet': [35, 29],\n",
       " 'days': [176, 101],\n",
       " 'row': [5, 6],\n",
       " 'ever': [393, 152],\n",
       " 'message': [75, 45],\n",
       " 'schedule': [13, 11],\n",
       " 'share': [107, 75],\n",
       " 'thoughts': [212, 102],\n",
       " 'therapist': [116, 59],\n",
       " 'though': [220, 117],\n",
       " 'coordinator': [2, 3],\n",
       " 'super': [28, 23],\n",
       " 'safety': [6, 7],\n",
       " 'reopen': [1, 2],\n",
       " 'open': [126, 69],\n",
       " 'pick': [83, 56],\n",
       " 'scab': [1, 2],\n",
       " 'skin': [16, 15],\n",
       " 'knife': [12, 11],\n",
       " 'hard': [416, 148],\n",
       " 'almost': [123, 82],\n",
       " 'impossible': [33, 26],\n",
       " 'long': [364, 155],\n",
       " 'hate': [171, 103],\n",
       " 'managers': [1, 2],\n",
       " 'use': [344, 155],\n",
       " 'manager': [7, 6],\n",
       " 'felt': [227, 107],\n",
       " 'frustrate': [32, 25],\n",
       " 'best': [394, 167],\n",
       " 'understand': [449, 154],\n",
       " 'easy': [113, 76],\n",
       " 'meet': [230, 107],\n",
       " 'new': [316, 134],\n",
       " 'kind': [267, 115],\n",
       " 'same': [427, 176],\n",
       " 'alone': [185, 112],\n",
       " 'until': [235, 121],\n",
       " 'shes': [162, 71],\n",
       " 'job': [301, 119],\n",
       " 'realize': [159, 84],\n",
       " 'sometimes': [248, 114],\n",
       " 'wrong': [129, 86],\n",
       " 'anymore': [40, 29],\n",
       " 'worry': [120, 70],\n",
       " 'stay': [208, 103],\n",
       " 'safe': [32, 24],\n",
       " 'while': [280, 139],\n",
       " 'listen': [167, 98],\n",
       " 'deserve': [128, 72],\n",
       " 'everyone': [271, 130],\n",
       " 'ear': [8, 8],\n",
       " 'thank': [227, 92],\n",
       " 'honest': [52, 39],\n",
       " 'hold': [138, 89],\n",
       " 'glad': [156, 70],\n",
       " 'pretty': [258, 118],\n",
       " 'fuck': [379, 121],\n",
       " 'amaze': [63, 46],\n",
       " 'internet': [58, 45],\n",
       " 'didnt': [420, 154],\n",
       " 'disappear': [11, 10],\n",
       " 'middle': [20, 20],\n",
       " 'our': [275, 113],\n",
       " 'name': [66, 46],\n",
       " 'kitty': [2, 2],\n",
       " 'able': [358, 143],\n",
       " 'around': [348, 156],\n",
       " 'important': [125, 64],\n",
       " 'strong': [97, 63],\n",
       " 'wonder': [72, 56],\n",
       " 'theres': [348, 122],\n",
       " 'service': [39, 31],\n",
       " 'area': [25, 21],\n",
       " 'reach': [143, 74],\n",
       " 'burden': [46, 35],\n",
       " 'doctor': [176, 71],\n",
       " 'option': [46, 36],\n",
       " 'happen': [293, 150],\n",
       " 'alive': [68, 50],\n",
       " 'wont': [302, 143],\n",
       " 'anyone': [225, 129],\n",
       " 'run': [94, 65],\n",
       " 'buy': [62, 44],\n",
       " 'back': [439, 163],\n",
       " 'headbutt': [1, 2],\n",
       " 'spasm': [2, 2],\n",
       " 'imagine': [138, 76],\n",
       " 'scary': [33, 26],\n",
       " 'difficult': [110, 65],\n",
       " 'deal': [272, 111],\n",
       " 'wound': [7, 7],\n",
       " 'bleed': [12, 10],\n",
       " 'cat': [20, 14],\n",
       " 'king': [4, 5],\n",
       " 'isnt': [355, 141],\n",
       " 'hes': [111, 57],\n",
       " 'his': [309, 111],\n",
       " 'animals': [11, 9],\n",
       " 'sad': [111, 76],\n",
       " 'comfort': [39, 35],\n",
       " 'experience': [230, 105],\n",
       " 'miss': [104, 68],\n",
       " 'great': [236, 116],\n",
       " 'support': [178, 86],\n",
       " 'worth': [290, 118],\n",
       " 'stressful': [19, 19],\n",
       " 'mom': [77, 52],\n",
       " 'favorite': [28, 26],\n",
       " 'app': [4, 4],\n",
       " 'game': [92, 50],\n",
       " 'play': [159, 89],\n",
       " 'mention': [91, 61],\n",
       " 'too': [486, 178],\n",
       " 'addict': [14, 12],\n",
       " 'those': [440, 162],\n",
       " 'distractions': [3, 4],\n",
       " 'begin': [86, 62],\n",
       " 'tendency': [1, 2],\n",
       " 'severe': [29, 24],\n",
       " 'comfortable': [48, 35],\n",
       " 'incredibly': [44, 29],\n",
       " 'brave': [17, 15],\n",
       " 'completely': [130, 85],\n",
       " 'weird': [34, 30],\n",
       " 'call': [314, 119],\n",
       " 'beneficial': [11, 10],\n",
       " 'tough': [59, 40],\n",
       " 'ahead': [56, 39],\n",
       " 'their': [496, 162],\n",
       " 'procedure': [2, 3],\n",
       " 'exhaustion': [47, 34],\n",
       " 'runaround': [1, 2],\n",
       " 'stuff': [156, 77],\n",
       " 'part': [230, 124],\n",
       " 'lead': [68, 54],\n",
       " 'post': [271, 121],\n",
       " 'guarantee': [35, 32],\n",
       " 'notice': [65, 50],\n",
       " 'already': [165, 97],\n",
       " 'wait': [124, 81],\n",
       " 'months': [133, 76],\n",
       " 'show': [158, 89],\n",
       " 'rejection': [8, 7],\n",
       " 'put': [333, 147],\n",
       " 'whatever': [176, 92],\n",
       " 'ago': [82, 63],\n",
       " 'between': [92, 67],\n",
       " 'manage': [65, 53],\n",
       " 'during': [69, 45],\n",
       " 'unfair': [9, 10],\n",
       " 'against': [64, 51],\n",
       " 'naturally': [11, 11],\n",
       " 'suicidal': [165, 94],\n",
       " 'word': [118, 73],\n",
       " 'change': [359, 144],\n",
       " 'offer': [133, 72],\n",
       " 'stop': [313, 132],\n",
       " 'hot': [16, 14],\n",
       " 'minute': [15, 15],\n",
       " 'quit': [27, 26],\n",
       " 'ask': [350, 141],\n",
       " 'off': [308, 146],\n",
       " 'most': [453, 168],\n",
       " 'strange': [14, 14],\n",
       " 'suicide': [351, 161],\n",
       " 'which': [321, 140],\n",
       " 'cause': [168, 84],\n",
       " 'admit': [40, 35],\n",
       " 'spring': [3, 4],\n",
       " 'sock': [3, 3],\n",
       " 'pass': [75, 57],\n",
       " 'many': [436, 159],\n",
       " 'these': [365, 152],\n",
       " 'inability': [11, 11],\n",
       " 'source': [15, 14],\n",
       " 'frustration': [10, 10],\n",
       " 'arent': [200, 112],\n",
       " 'paramedic': [1, 2],\n",
       " 'position': [36, 28],\n",
       " 'disgust': [6, 7],\n",
       " 'totally': [69, 51],\n",
       " 'crazy': [20, 17],\n",
       " 'curious': [17, 15],\n",
       " 'whats': [218, 84],\n",
       " 'failure': [25, 19],\n",
       " 'scar': [48, 35],\n",
       " 'often': [119, 81],\n",
       " 'itll': [59, 40],\n",
       " 'permanently': [7, 6],\n",
       " 'stick': [114, 76],\n",
       " 'where': [462, 178],\n",
       " 'end': [415, 167],\n",
       " 'atleast': [4, 5],\n",
       " 'strongly': [18, 15],\n",
       " 'either': [121, 86],\n",
       " 'beautiful': [49, 39],\n",
       " 'shitty': [39, 21],\n",
       " 'weakness': [16, 15],\n",
       " 'hand': [59, 46],\n",
       " 'tear': [25, 20],\n",
       " 'down': [325, 137],\n",
       " 'own': [286, 129],\n",
       " 'wish': [247, 126],\n",
       " 'badly': [23, 19],\n",
       " 'similar': [125, 86],\n",
       " 'hopeless': [46, 44],\n",
       " 'light': [45, 34],\n",
       " 'please': [307, 103],\n",
       " 'rough': [44, 26],\n",
       " 'hurt': [62, 47],\n",
       " 'heart': [76, 54],\n",
       " 'free': [131, 88],\n",
       " 'friends': [474, 162],\n",
       " 'boyfriend': [44, 30],\n",
       " 'force': [89, 59],\n",
       " 'tie': [18, 16],\n",
       " 'lose': [278, 128],\n",
       " 'friend': [244, 116],\n",
       " 'release': [19, 15],\n",
       " 'whenever': [54, 43],\n",
       " 'overwhelm': [38, 27],\n",
       " 'before': [300, 144],\n",
       " 'sight': [6, 7],\n",
       " 'blood': [18, 13],\n",
       " 'moment': [66, 50],\n",
       " 'wind': [12, 12],\n",
       " 'heal': [31, 25],\n",
       " 'after': [337, 150],\n",
       " 'therell': [2, 3],\n",
       " 'nasty': [9, 9],\n",
       " 'ill': [337, 140],\n",
       " 'entitle': [4, 4],\n",
       " 'extremely': [50, 39],\n",
       " 'believe': [360, 145],\n",
       " 'read': [280, 126],\n",
       " 'urge': [36, 28],\n",
       " 'draw': [38, 34],\n",
       " 'markers': [1, 2],\n",
       " 'produce': [13, 12],\n",
       " 'calm': [27, 21],\n",
       " 'grind': [12, 10],\n",
       " 'themselves': [77, 52],\n",
       " 'pen': [7, 8],\n",
       " 'happy': [292, 128],\n",
       " 'awhile': [12, 12],\n",
       " 'perhaps': [91, 47],\n",
       " 'unhappy': [41, 32],\n",
       " 'postpone': [4, 5],\n",
       " 'follow': [75, 52],\n",
       " 'articulate': [7, 8],\n",
       " 'well': [304, 133],\n",
       " 'comment': [84, 49],\n",
       " 'impact': [39, 23],\n",
       " 'guess': [184, 101],\n",
       " 'relate': [75, 58],\n",
       " 'others': [218, 117],\n",
       " 'fairly': [33, 25],\n",
       " 'rare': [13, 12],\n",
       " 'strangers': [17, 14],\n",
       " 'genuinely': [34, 27],\n",
       " 'humans': [29, 22],\n",
       " 'hardest': [27, 23],\n",
       " 'appreciate': [109, 68],\n",
       " 'besides': [20, 18],\n",
       " 'parent': [247, 112],\n",
       " 'half': [56, 45],\n",
       " 'hour': [42, 35],\n",
       " 'every': [364, 144],\n",
       " 'two': [216, 121],\n",
       " 'enough': [287, 136],\n",
       " 'search': [39, 31],\n",
       " 'view': [43, 37],\n",
       " 'depression': [426, 144],\n",
       " 'ideations': [1, 2],\n",
       " 'embarrass': [16, 14],\n",
       " 'big': [142, 84],\n",
       " 'ashamed': [20, 19],\n",
       " 'must': [132, 64],\n",
       " 'breathe': [10, 10],\n",
       " 'beautifully': [5, 6],\n",
       " 'struggle': [105, 71],\n",
       " 'worthy': [9, 10],\n",
       " 'romanticize': [3, 4],\n",
       " 'else': [215, 124],\n",
       " 'decisive': [1, 2],\n",
       " 'wouldnt': [212, 123],\n",
       " 'solve': [41, 36],\n",
       " 'brighten': [3, 4],\n",
       " 'day': [358, 160],\n",
       " 'without': [261, 127],\n",
       " 'fully': [24, 23],\n",
       " 'capable': [29, 27],\n",
       " 'push': [94, 67],\n",
       " 'trust': [145, 77],\n",
       " 'theyre': [156, 67],\n",
       " 'numerous': [8, 8],\n",
       " 'step': [129, 66],\n",
       " 'zone': [11, 9],\n",
       " 'such': [161, 102],\n",
       " 'tattoo': [1, 2],\n",
       " 'spoil': [5, 6],\n",
       " 'hobby': [20, 17],\n",
       " 'blow': [34, 32],\n",
       " 'total': [24, 19],\n",
       " 'stranger': [27, 22],\n",
       " 'write': [221, 110],\n",
       " 'next': [136, 83],\n",
       " 'reddit': [60, 44],\n",
       " 'resources': [36, 23],\n",
       " 'sub': [19, 16],\n",
       " 'overcome': [50, 30],\n",
       " 'nice': [110, 72],\n",
       " 'motivate': [27, 22],\n",
       " 'refresh': [7, 8],\n",
       " 'plenty': [43, 35],\n",
       " 'slump': [2, 3],\n",
       " 'wanna': [27, 24],\n",
       " 'bone': [5, 6],\n",
       " 'demand': [10, 10],\n",
       " 'wrap': [11, 10],\n",
       " 'seriously': [70, 54],\n",
       " 'different': [214, 105],\n",
       " 'social': [147, 79],\n",
       " 'encourage': [23, 19],\n",
       " 'journey': [11, 10],\n",
       " 'surpass': [3, 4],\n",
       " 'truly': [146, 84],\n",
       " 'admire': [7, 7],\n",
       " 'prefer': [21, 17],\n",
       " 'herbs': [1, 2],\n",
       " 'natural': [34, 24],\n",
       " 'remedy': [6, 7],\n",
       " 'intrigue': [3, 4],\n",
       " 'away': [213, 115],\n",
       " 'nothing': [326, 143],\n",
       " 'fix': [82, 59],\n",
       " 'desire': [42, 35],\n",
       " 'uplift': [2, 3],\n",
       " 'spare': [14, 13],\n",
       " 'ten': [17, 18],\n",
       " 'bother': [69, 44],\n",
       " 'few': [341, 152],\n",
       " 'nights': [18, 13],\n",
       " 'weight': [43, 33],\n",
       " 'break': [214, 115],\n",
       " 'place': [238, 123],\n",
       " 'easier': [81, 58],\n",
       " 'negative': [90, 48],\n",
       " 'weigh': [18, 14],\n",
       " 'rat': [7, 7],\n",
       " 'afraid': [90, 56],\n",
       " 'theyll': [65, 40],\n",
       " 'minutes': [33, 28],\n",
       " 'download': [9, 8],\n",
       " 'weeks': [57, 48],\n",
       " 'emotions': [45, 37],\n",
       " 'biggest': [38, 31],\n",
       " 'issue': [116, 66],\n",
       " 'nervous': [15, 12],\n",
       " 'clinics': [6, 6],\n",
       " 'sincere': [4, 5],\n",
       " 'whether': [91, 62],\n",
       " 'himself': [32, 29],\n",
       " 'youd': [126, 67],\n",
       " 'despite': [44, 36],\n",
       " 'claim': [17, 17],\n",
       " 'contact': [91, 54],\n",
       " 'okay': [59, 40],\n",
       " 'necessarily': [31, 28],\n",
       " 'age': [68, 52],\n",
       " 'emotional': [50, 39],\n",
       " 'upset': [70, 52],\n",
       " 'apathetic': [5, 6],\n",
       " 'everything': [322, 146],\n",
       " 'actively': [19, 19],\n",
       " 'society': [45, 32],\n",
       " 'certain': [58, 41],\n",
       " 'matter': [228, 118],\n",
       " 'societys': [1, 2],\n",
       " 'control': [98, 58],\n",
       " 'situations': [40, 32],\n",
       " 'lazy': [12, 11],\n",
       " 'rewire': [8, 5],\n",
       " 'cbt': [8, 6],\n",
       " 'cognitive': [14, 10],\n",
       " 'behavioral': [7, 7],\n",
       " 'base': [43, 36],\n",
       " 'belief': [7, 7],\n",
       " 'patients': [6, 7],\n",
       " 'pattern': [11, 10],\n",
       " 'self': [117, 70],\n",
       " 'mindset': [12, 11],\n",
       " 'action': [47, 32],\n",
       " 'morning': [20, 18],\n",
       " 'slowly': [38, 30],\n",
       " 'process': [30, 27],\n",
       " 'suck': [83, 57],\n",
       " 'complete': [41, 36],\n",
       " 'task': [11, 12],\n",
       " 'course': [74, 48],\n",
       " 'youll': [328, 108],\n",
       " 'hopefully': [56, 39],\n",
       " 'tend': [49, 37],\n",
       " 'yet': [108, 73],\n",
       " 'rarely': [17, 17],\n",
       " 'anybody': [25, 16],\n",
       " 'hearts': [5, 6],\n",
       " 'toughest': [3, 4],\n",
       " 'everybody': [22, 14],\n",
       " 'brunt': [2, 3],\n",
       " 'dad': [90, 51],\n",
       " 'today': [66, 50],\n",
       " 'grandmas': [3, 3],\n",
       " 'house': [76, 55],\n",
       " 'sit': [112, 69],\n",
       " 'alot': [4, 5],\n",
       " 'closet': [2, 3],\n",
       " 'shoot': [43, 35],\n",
       " 'gun': [50, 42],\n",
       " 'nap': [1, 2],\n",
       " 'over': [441, 173],\n",
       " 'bump': [5, 6],\n",
       " 'hawaii': [1, 2],\n",
       " 'old': [161, 99],\n",
       " 'music': [36, 29],\n",
       " 'beach': [5, 6],\n",
       " 'year': [211, 102],\n",
       " 'him': [321, 126],\n",
       " 'guy': [143, 85],\n",
       " 'again': [151, 93],\n",
       " 'swim': [16, 10],\n",
       " 'errands': [2, 3],\n",
       " 'lap': [2, 3],\n",
       " 'school': [277, 116],\n",
       " 'stomach': [14, 14],\n",
       " 'kinda': [57, 39],\n",
       " 'uneasy': [2, 3],\n",
       " 'fell': [27, 20],\n",
       " 'asleep': [5, 6],\n",
       " 'puke': [2, 3],\n",
       " 'bottle': [20, 20],\n",
       " 'probally': [1, 2],\n",
       " 'disolved': [1, 2],\n",
       " 'very': [285, 102],\n",
       " 'gonna': [76, 41],\n",
       " 'nowedit': [1, 2],\n",
       " 'log': [6, 6],\n",
       " 'finish': [35, 30],\n",
       " 'bed': [66, 47],\n",
       " 'family': [277, 134],\n",
       " 'dinner': [5, 6],\n",
       " 'tonight': [23, 20],\n",
       " 'another': [244, 136],\n",
       " 'night': [74, 52],\n",
       " 'dive': [5, 6],\n",
       " 'nothingness': [3, 3],\n",
       " 'money': [136, 81],\n",
       " 'baby': [32, 22],\n",
       " 'commit': [114, 72],\n",
       " 'slim': [4, 5],\n",
       " 'tiniest': [1, 2],\n",
       " 'enjoy': [174, 83],\n",
       " 'link': [26, 22],\n",
       " 'promise': [75, 49],\n",
       " 'acknowledge': [17, 15],\n",
       " 'hardly': [23, 22],\n",
       " 'worst': [84, 64],\n",
       " 'fall': [89, 67],\n",
       " 'expect': [69, 45],\n",
       " 'reality': [35, 31],\n",
       " 'powerless': [2, 3],\n",
       " 'shouldnt': [95, 65],\n",
       " 'remove': [11, 11],\n",
       " 'anonymity': [3, 4],\n",
       " 'primary': [9, 9],\n",
       " 'reason': [327, 155],\n",
       " 'instead': [96, 70],\n",
       " 'hotline': [39, 19],\n",
       " 'havent': [269, 116],\n",
       " 'escape': [23, 21],\n",
       " 'stand': [66, 53],\n",
       " 'picture': [27, 20],\n",
       " 'cancel': [7, 7],\n",
       " 'preorder': [1, 2],\n",
       " 'pending': [2, 3],\n",
       " 'camera': [3, 4],\n",
       " 'tank': [3, 4],\n",
       " 'excessive': [7, 7],\n",
       " 'upper': [10, 9],\n",
       " 'gastrointestinal': [5, 6],\n",
       " 'gas': [6, 7],\n",
       " 'onto': [23, 22],\n",
       " 'saw': [72, 47],\n",
       " 'pay': [123, 73],\n",
       " 'policy': [3, 3],\n",
       " 'whole': [135, 81],\n",
       " 'irony': [3, 4],\n",
       " 'willingness': [4, 5],\n",
       " 'ultimate': [10, 10],\n",
       " 'price': [6, 7],\n",
       " 'belittle': [4, 5],\n",
       " 'outpouring': [1, 2],\n",
       " 'sympathy': [6, 7],\n",
       " 'death': [115, 75],\n",
       " 'decease': [1, 2],\n",
       " 'real': [178, 110],\n",
       " 'quite': [116, 67],\n",
       " 'blind': [7, 7],\n",
       " 'communal': [1, 2],\n",
       " 'grieve': [5, 6],\n",
       " 'cathartic': [7, 6],\n",
       " 'mostly': [36, 34],\n",
       " 'exist': [37, 36],\n",
       " 'acquaintances': [8, 8],\n",
       " 'useful': [16, 14],\n",
       " 'dead': [55, 43],\n",
       " 'rate': [7, 6],\n",
       " 'forget': [66, 57],\n",
       " 'narrow': [7, 7],\n",
       " 'lifetime': [11, 12],\n",
       " 'attempt': [102, 67],\n",
       " 'clearer': [1, 2],\n",
       " 'estimate': [1, 2],\n",
       " 'average': [19, 19],\n",
       " 'suicides': [9, 10],\n",
       " 'early': [30, 27],\n",
       " 'bear': [52, 42],\n",
       " 'million': [17, 16],\n",
       " 'birth': [9, 9],\n",
       " 'present': [31, 24],\n",
       " 'far': [176, 94],\n",
       " 'certainly': [79, 46],\n",
       " 'deeply': [17, 17],\n",
       " 'walk': [108, 80],\n",
       " 'romantic': [7, 8],\n",
       " 'partner': [26, 19],\n",
       " 'tolerate': [5, 6],\n",
       " 'couple': [112, 66],\n",
       " 'online': [58, 40],\n",
       " 'confess': [4, 5],\n",
       " 'sense': [90, 72],\n",
       " 'generally': [27, 21],\n",
       " 'insufferable': [1, 2],\n",
       " 'mistake': [48, 39],\n",
       " 'remember': [202, 110],\n",
       " 'date': [61, 40],\n",
       " 'personality': [23, 19],\n",
       " 'match': [13, 14],\n",
       " 'score': [4, 5],\n",
       " 'high': [77, 45],\n",
       " 'probably': [319, 133],\n",
       " 'true': [70, 54],\n",
       " 'last': [252, 122],\n",
       " 'none': [42, 34],\n",
       " 'forward': [52, 43],\n",
       " 'depersonalization': [10, 9],\n",
       " 'pit': [6, 6],\n",
       " 'turn': [176, 93],\n",
       " 'salvation': [2, 3],\n",
       " 'head': [126, 77],\n",
       " 'drug': [170, 80],\n",
       " 'withdrawal': [6, 7],\n",
       " 'syndrome': [9, 9],\n",
       " 'freight': [1, 2],\n",
       " 'cannot': [103, 56],\n",
       " 'bring': [179, 98],\n",
       " 'hopelessness': [5, 6],\n",
       " 'hasnt': [44, 37],\n",
       " 'beat': [50, 40],\n",
       " 'years': [367, 165],\n",
       " 'sign': [40, 35],\n",
       " 'second': [83, 57],\n",
       " 'strive': [12, 10],\n",
       " 'survive': [44, 31],\n",
       " 'wear': [31, 26],\n",
       " 'badge': [1, 2],\n",
       " 'somehow': [33, 26],\n",
       " 'depress': [190, 86],\n",
       " 'mood': [361, 122],\n",
       " 'actual': [28, 29],\n",
       " 'yes': [30, 23],\n",
       " 'clear': [50, 36],\n",
       " 'debt': [26, 20],\n",
       " 'ways': [77, 58],\n",
       " 'finacial': [1, 2],\n",
       " 'aid': [13, 12],\n",
       " 'university': [30, 24],\n",
       " 'situation': [178, 102],\n",
       " 'determine': [19, 19],\n",
       " 'sadness': [16, 16],\n",
       " 'leave': [310, 147],\n",
       " 'consume': [11, 11],\n",
       " 'destroy': [45, 34],\n",
       " 'discover': [34, 25],\n",
       " 'anxiety': [169, 87],\n",
       " 'mental': [405, 119],\n",
       " 'survivor': [2, 3],\n",
       " 'nameless': [2, 3],\n",
       " 'charge': [17, 17],\n",
       " 'millions': [7, 8],\n",
       " 'world': [283, 135],\n",
       " 'grow': [85, 56],\n",
       " 'planet': [16, 13],\n",
       " 'slow': [30, 21],\n",
       " 'girl': [106, 61],\n",
       " 'company': [26, 24],\n",
       " 'value': [52, 30],\n",
       " 'load': [19, 17],\n",
       " 'skills': [20, 20],\n",
       " 'couldnt': [119, 71],\n",
       " 'choose': [99, 56],\n",
       " 'choice': [51, 37],\n",
       " 'medical': [60, 41],\n",
       " 'condition': [30, 24],\n",
       " 'sicknesses': [1, 2],\n",
       " 'learn': [234, 98],\n",
       " 'join': [41, 32],\n",
       " 'army': [5, 3],\n",
       " 'sink': [11, 11],\n",
       " 'deeper': [13, 11],\n",
       " 'advice': [145, 84],\n",
       " 'gotta': [44, 25],\n",
       " 'question': [93, 64],\n",
       " 'goal': [25, 20],\n",
       " 'apart': [22, 19],\n",
       " 'culture': [7, 6],\n",
       " 'sex': [38, 22],\n",
       " 'fail': [102, 61],\n",
       " 'female': [12, 13],\n",
       " 'man': [87, 56],\n",
       " 'roommate': [7, 7],\n",
       " 'college': [125, 65],\n",
       " 'span': [5, 5],\n",
       " 'sleep': [73, 46],\n",
       " 'stab': [6, 5],\n",
       " 'lie': [80, 54],\n",
       " 'fact': [136, 86],\n",
       " 'possible': [85, 62],\n",
       " 'woman': [45, 33],\n",
       " 'desperation': [5, 5],\n",
       " 'direction': [22, 20],\n",
       " 'low': [48, 40],\n",
       " 'desperate': [13, 14],\n",
       " 'least': [259, 124],\n",
       " 'future': [99, 61],\n",
       " 'remain': [22, 22],\n",
       " 'hell': [70, 52],\n",
       " 'twice': [26, 21],\n",
       " 'past': [138, 82],\n",
       " 'outside': [68, 45],\n",
       " 'terrify': [15, 14],\n",
       " 'small': [93, 62],\n",
       " 'coffee': [12, 11],\n",
       " 'amusement': [2, 3],\n",
       " 'whereas': [3, 4],\n",
       " 'attention': [72, 56],\n",
       " 'drag': [21, 19],\n",
       " 'soon': [69, 50],\n",
       " 'faith': [32, 19],\n",
       " 'somewhat': [38, 36],\n",
       " 'cope': [32, 25],\n",
       " 'require': [40, 33],\n",
       " 'prove': [60, 39],\n",
       " 'decent': [33, 29],\n",
       " 'unwanted': [3, 4],\n",
       " 'surprise': [38, 33],\n",
       " 'situate': [1, 2],\n",
       " 'speak': [108, 71],\n",
       " 'suffer': [108, 70],\n",
       " 'perfect': [47, 27],\n",
       " 'become': [227, 111],\n",
       " 'unhappier': [1, 2],\n",
       " 'spend': [186, 98],\n",
       " 'build': [75, 52],\n",
       " 'irritable': [189, 78],\n",
       " 'usually': [101, 66],\n",
       " 'vent': [41, 24],\n",
       " 'seek': [85, 58],\n",
       " 'point': [296, 129],\n",
       " 'problems': [149, 93],\n",
       " 'problem': [129, 85],\n",
       " 'insist': [9, 8],\n",
       " 'once': [174, 101],\n",
       " 'relationship': [120, 55],\n",
       " 'distant': [10, 11],\n",
       " 'government': [8, 9],\n",
       " 'assistance': [10, 9],\n",
       " 'assume': [54, 44],\n",
       " 'luck': [51, 36],\n",
       " 'express': [32, 25],\n",
       " 'ourselves': [22, 20],\n",
       " 'first': [315, 145],\n",
       " 'battle': [31, 27],\n",
       " 'relieve': [17, 13],\n",
       " 'suggest': [83, 50],\n",
       " 'fill': [44, 34],\n",
       " 'page': [21, 17],\n",
       " 'christmas': [6, 6],\n",
       " 'repetitive': [6, 3],\n",
       " 'fun': [62, 46],\n",
       " 'mini': [2, 3],\n",
       " 'entertain': [12, 12],\n",
       " 'fly': [10, 9],\n",
       " 'fit': [47, 34],\n",
       " 'fight': [115, 64],\n",
       " 'sedate': [18, 16],\n",
       " 'state': [96, 63],\n",
       " 'room': [57, 43],\n",
       " 'mate': [7, 4],\n",
       " 'forever': [30, 27],\n",
       " 'save': [87, 59],\n",
       " 'continue': [90, 54],\n",
       " 'communicate': [15, 14],\n",
       " 'decide': [151, 90],\n",
       " 'grocery': [6, 7],\n",
       " 'drive': [85, 63],\n",
       " 'preferably': [3, 4],\n",
       " 'slightly': [13, 14],\n",
       " 'positive': [110, 72],\n",
       " 'shame': [32, 27],\n",
       " 'progressively': [3, 3],\n",
       " 'worse': [119, 81],\n",
       " 'postivie': [1, 2],\n",
       " 'harm': [56, 38],\n",
       " 'incredible': [14, 13],\n",
       " 'constantly': [36, 31],\n",
       " 'check': [94, 60],\n",
       " 'account': [22, 20],\n",
       " 'harder': [40, 36],\n",
       " 'career': [36, 27],\n",
       " 'field': [25, 24],\n",
       " 'opportunity': [30, 22],\n",
       " 'shove': [5, 6],\n",
       " 'side': [97, 61],\n",
       " 'vulnerable': [5, 6],\n",
       " 'reveal': [6, 6],\n",
       " 'boredom': [1, 2],\n",
       " 'horrendous': [3, 4],\n",
       " 'challenge': [29, 27],\n",
       " 'rid': [34, 29],\n",
       " 'hobbies': [18, 18],\n",
       " 'busy': [31, 26],\n",
       " 'conclusion': [13, 14],\n",
       " 'rapidly': [4, 4],\n",
       " 'sport': [13, 12],\n",
       " 'although': [60, 46],\n",
       " 'loonly': [1, 2],\n",
       " 'girlfriend': [64, 46],\n",
       " 'whom': [20, 14],\n",
       " 'figure': [114, 76],\n",
       " 'personal': [62, 45],\n",
       " 'attitude': [12, 10],\n",
       " 'proper': [19, 18],\n",
       " 'asshole': [9, 9],\n",
       " 'beliefe': [1, 2],\n",
       " 'paul': [2, 3],\n",
       " 'hospital': [97, 46],\n",
       " 'addition': [10, 6],\n",
       " 'struggeling': [2, 2],\n",
       " 'due': [52, 34],\n",
       " 'furthermore': [2, 2],\n",
       " 'shit': [209, 101],\n",
       " 'stronger': [45, 37],\n",
       " 'move': [194, 108],\n",
       " 'dependent': [5, 6],\n",
       " 'guitar': [13, 10],\n",
       " 'depressive': [6, 7],\n",
       " 'gym': [19, 13],\n",
       " 'favourite': [10, 9],\n",
       " 'song': [16, 16],\n",
       " 'remind': [55, 42],\n",
       " 'smth': [1, 2],\n",
       " 'exactly': [101, 78],\n",
       " 'under': [68, 51],\n",
       " 'constant': [30, 26],\n",
       " 'intense': [4, 5],\n",
       " 'nerve': [6, 7],\n",
       " 'partly': [8, 6],\n",
       " 'psychological': [15, 12],\n",
       " 'extremly': [2, 2],\n",
       " 'painkillers': [2, 3],\n",
       " 'period': [22, 21],\n",
       " 'easily': [29, 26],\n",
       " 'recognizable': [2, 3],\n",
       " 'twist': [13, 14],\n",
       " 'frequently': [10, 9],\n",
       " 'aggressive': [2, 3],\n",
       " 'aswell': [2, 3],\n",
       " 'drowsy': [1, 2],\n",
       " 'hatred': [7, 8],\n",
       " 'rest': [106, 69],\n",
       " 'unguilty': [1, 2],\n",
       " 'nearly': [38, 26],\n",
       " 'several': [41, 28],\n",
       " 'treatments': [8, 7],\n",
       " 'selfconfident': [1, 2],\n",
       " 'recognize': [26, 21],\n",
       " 'character': [19, 14],\n",
       " 'consist': [4, 5],\n",
       " 'suicde': [1, 2],\n",
       " 'childhood': [11, 11],\n",
       " 'wasnt': [145, 90],\n",
       " 'buillied': [1, 2],\n",
       " 'mother': [100, 64],\n",
       " 'breast': [2, 3],\n",
       " 'convienced': [1, 2],\n",
       " 'otherwise': [23, 22],\n",
       " 'healthy': [30, 29],\n",
       " 'respond': [57, 47],\n",
       " 'above': [34, 25],\n",
       " 'gentler': [1, 2],\n",
       " 'instincts': [4, 4],\n",
       " 'center': [21, 15],\n",
       " 'regardless': [16, 13],\n",
       " 'giggle': [1, 2],\n",
       " 'detail': [25, 21],\n",
       " 'shine': [4, 5],\n",
       " 'physical': [48, 31],\n",
       " 'pretend': [28, 23],\n",
       " 'nothings': [3, 3],\n",
       " 'pat': [1, 2],\n",
       " 'answer': [99, 63],\n",
       " 'serious': [55, 46],\n",
       " 'silence': [5, 5],\n",
       " 'interruption': [1, 2],\n",
       " 'judgement': [6, 6],\n",
       " 'glib': [1, 2],\n",
       " 'truth': [41, 32],\n",
       " 'sympathetic': [7, 7],\n",
       " 'likely': [87, 57],\n",
       " 'ruminate': [2, 3],\n",
       " 'underlie': [8, 7],\n",
       " 'tone': [5, 6],\n",
       " 'clue': [12, 12],\n",
       " 'honor': [7, 7],\n",
       " 'throughout': [22, 20],\n",
       " 'werent': [34, 30],\n",
       " 'secure': [4, 5],\n",
       " 'full': [84, 63],\n",
       " 'illness': [62, 36],\n",
       " 'add': [48, 37],\n",
       " 'sing': [12, 12],\n",
       " 'ridiculous': [10, 11],\n",
       " 'songs': [4, 5],\n",
       " 'watch': [138, 83],\n",
       " 'together': [78, 59],\n",
       " 'cable': [2, 3],\n",
       " 'phone': [45, 32],\n",
       " 'episodes': [12, 8],\n",
       " 'news': [16, 12],\n",
       " 'stories': [22, 18],\n",
       " 'interest': [178, 86],\n",
       " 'quietly': [5, 6],\n",
       " 'bluetooth': [1, 2],\n",
       " 'ordinary': [4, 4],\n",
       " 'possibly': [40, 34],\n",
       " 'virtual': [4, 5],\n",
       " 'alienate': [6, 6],\n",
       " 'humanity': [10, 9],\n",
       " 'number': [51, 38],\n",
       " 'odds': [13, 14],\n",
       " 'bore': [24, 20],\n",
       " 'ignore': [54, 41],\n",
       " 'owe': [19, 19],\n",
       " 'women': [47, 26],\n",
       " 'ones': [114, 60],\n",
       " 'females': [3, 4],\n",
       " 'prize': [1, 2],\n",
       " 'unless': [55, 43],\n",
       " 'luckiest': [1, 2],\n",
       " 'bonehead': [1, 2],\n",
       " 'wipe': [6, 7],\n",
       " 'gently': [1, 2],\n",
       " 'coax': [1, 2],\n",
       " 'devil': [2, 3],\n",
       " 'incarnate': [1, 2],\n",
       " 'michael': [2, 3],\n",
       " 'jackson': [1, 2],\n",
       " 'close': [120, 83],\n",
       " 'normal': [88, 60],\n",
       " 'nose': [6, 5],\n",
       " 'cartilage': [1, 2],\n",
       " 'fine': [54, 40],\n",
       " 'face': [90, 62],\n",
       " 'cover': [22, 21],\n",
       " 'scaly': [1, 2],\n",
       " 'food': [40, 35],\n",
       " 'rude': [9, 10],\n",
       " 'smile': [59, 39],\n",
       " 'pin': [3, 4],\n",
       " 'hop': [66, 51],\n",
       " 'six': [27, 19],\n",
       " 'email': [27, 18],\n",
       " 'text': [31, 26],\n",
       " 'heres': [34, 27],\n",
       " 'herself': [18, 15],\n",
       " 'blank': [6, 7],\n",
       " 'wtf': [3, 4],\n",
       " 'media': [9, 7],\n",
       " 'satisfaction': [12, 12],\n",
       " 'response': [42, 28],\n",
       " 'type': [98, 63],\n",
       " 'baffle': [4, 4],\n",
       " 'outta': [4, 5],\n",
       " 'wheres': [1, 2],\n",
       " 'alarm': [4, 5],\n",
       " 'palsy': [1, 2],\n",
       " 'consider': [236, 105],\n",
       " 'organic': [2, 3],\n",
       " 'general': [48, 38],\n",
       " 'chem': [1, 2],\n",
       " 'panel': [2, 3],\n",
       " 'relatively': [17, 17],\n",
       " 'common': [50, 38],\n",
       " 'axis': [1, 2],\n",
       " 'balance': [8, 8],\n",
       " 'simple': [51, 46],\n",
       " 'endocrine': [1, 2],\n",
       " 'system': [25, 19],\n",
       " 'function': [18, 17],\n",
       " 'physician': [1, 2],\n",
       " 'improve': [77, 41],\n",
       " 'absolutely': [93, 57],\n",
       " 'contribute': [20, 19],\n",
       " 'die': [165, 97],\n",
       " 'case': [68, 47],\n",
       " 'responsible': [20, 17],\n",
       " 'happiness': [82, 45],\n",
       " 'blame': [60, 46],\n",
       " 'limit': [26, 21],\n",
       " 'comfortably': [1, 2],\n",
       " 'train': [36, 26],\n",
       " 'professional': [74, 42],\n",
       " 'helplessness': [2, 3],\n",
       " 'stress': [69, 46],\n",
       " 'pull': [88, 55],\n",
       " 'deep': [58, 38],\n",
       " 'confuse': [17, 14],\n",
       " 'latest': [2, 3],\n",
       " 'statement': [8, 9],\n",
       " 'drop': [61, 44],\n",
       " 'responses': [13, 11],\n",
       " 'guide': [18, 14],\n",
       " 'power': [35, 27],\n",
       " 'helpless': [7, 8],\n",
       " 'shut': [19, 16],\n",
       " 'trigger': [30, 24],\n",
       " 'helpful': [25, 21],\n",
       " 'kid': [121, 64],\n",
       " 'crowd': [3, 4],\n",
       " 'biologically': [3, 4],\n",
       " 'reproduce': [5, 4],\n",
       " 'genetics': [1, 2],\n",
       " 'methods': [18, 17],\n",
       " 'beyond': [44, 36],\n",
       " 'bright': [18, 17],\n",
       " ...}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocab_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라벨이 없는 test data를 예측하여 csv파일로 바꾸어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 1000) (0,)\n",
      "(150, 1000) (0,)\n"
     ]
    }
   ],
   "source": [
    "cv = Vectorizer(1000, tfidf = False, stop_word = True)\n",
    "cv.fit(x_train)\n",
    "term_docs_train = cv.transform(x_train)\n",
    "test_term_docs = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = NaiveBayes(smoothing = 1)\n",
    "NB.fit(term_docs_train, y_train)\n",
    "tes_pred = NB.predict(test_term_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "1 1\n",
      "2 2\n",
      "0 0\n",
      "3 3\n",
      "3 3\n",
      "3 3\n",
      "1 1\n",
      "3 1\n",
      "4 3\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "4 4\n",
      "4 4\n",
      "3 3\n",
      "4 4\n",
      "4 4\n",
      "1 1\n",
      "4 4\n",
      "0 0\n",
      "4 4\n",
      "3 3\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "3 0\n",
      "4 4\n",
      "1 1\n",
      "0 0\n",
      "3 4\n",
      "4 4\n",
      "1 1\n",
      "4 4\n",
      "0 0\n",
      "0 0\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "2 2\n",
      "4 4\n",
      "0 4\n",
      "4 4\n",
      "4 3\n",
      "1 1\n",
      "0 0\n",
      "4 4\n",
      "4 4\n",
      "0 0\n",
      "3 3\n",
      "0 0\n",
      "3 3\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "0 0\n",
      "0 0\n",
      "3 3\n",
      "1 4\n",
      "4 4\n",
      "4 4\n",
      "3 3\n",
      "4 4\n",
      "0 0\n",
      "3 3\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "3 3\n",
      "1 1\n",
      "4 4\n",
      "3 3\n",
      "3 1\n",
      "4 4\n",
      "4 4\n",
      "3 4\n",
      "1 1\n",
      "3 3\n",
      "4 4\n",
      "4 4\n",
      "1 1\n",
      "4 4\n",
      "4 4\n",
      "3 0\n",
      "4 4\n",
      "3 3\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "0 4\n",
      "4 4\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "4 4\n",
      "3 3\n",
      "1 1\n",
      "0 0\n",
      "4 4\n",
      "4 4\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "3 3\n",
      "4 4\n",
      "0 0\n",
      "4 4\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "2 2\n",
      "0 0\n",
      "4 4\n",
      "3 3\n",
      "4 4\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "0 0\n",
      "4 4\n",
      "4 4\n",
      "0 0\n",
      "3 3\n",
      "3 3\n",
      "4 4\n",
      "0 0\n",
      "4 4\n",
      "4 4\n",
      "3 3\n",
      "4 4\n",
      "0 0\n",
      "4 4\n",
      "4 4\n",
      "1 1\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_pred)):\n",
    "    print(tes_pred[i], test_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_pred).to_csv(\"2018310412.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

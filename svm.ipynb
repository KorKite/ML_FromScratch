{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove name entity\n",
    "from nltk.corpus import names\n",
    "all_names = set(names.words())\n",
    "# lemmaization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def letters_only(word):\n",
    "  return word.isalpha()\n",
    "\n",
    "# put all together to clean texts\n",
    "def clean_text(doc):\n",
    "    cleaned_doc = []\n",
    "    for word in doc.split(' '): # split doc. by blank (' ')\n",
    "        word = word.lower() # ABD -> abd\n",
    "  \n",
    "        if letters_only(word) and word not in all_names and len(word) > 2: # remove number and punc. and name entity\n",
    "            cleaned_doc.append(lemmatizer.lemmatize(word, \"v\"))\n",
    "    return ' '.join(cleaned_doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 150\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"TRAIN.csv\")\n",
    "test_df = pd.read_csv(\"TEST.csv\")\n",
    "x_train = train_df.iloc[:, 2]\n",
    "y_train = train_df.iloc[:,-1].astype(int)\n",
    "x_train = [clean_text(doc) for doc in x_train]\n",
    "x_test = test_df[\"Post\"]\n",
    "x_test = [clean_text(doc) for doc in x_test]\n",
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "class Vectorizer:\n",
    "    def __init__(self, max_len = 100000, tfidf = False, stop_word = False, max_df = float(\"inf\"),\n",
    "                 min_df = -float(\"inf\"), bigram = False):\n",
    "        self.max_len = max_len\n",
    "        self.tfidf = tfidf\n",
    "        self.stop_word = stop_word\n",
    "        self.max_df = max_df\n",
    "        self.ngram = bigram\n",
    "        self.min_df = min_df\n",
    "    \n",
    "    def vocab_(self):\n",
    "        return self.word_count\n",
    "    \n",
    "    def n_gram(self, text, n):\n",
    "        fi = text.split(\" \")\n",
    "        ret_list = [None for i in range(len(fi) -n +1)]\n",
    "        for i in range(len(fi) -n +1):\n",
    "            add = \"\"\n",
    "            for k in range(n-1):\n",
    "                add += fi[i + k]\n",
    "                add += \" \"\n",
    "            add += fi[i+n-1]\n",
    "            ret_list[i] = add\n",
    "        return ret_list\n",
    "    \n",
    "    \n",
    "    def make_vector(self, word_count):\n",
    "        self.word_count = word_count\n",
    "        sort_by_count = sorted(word_count.items(), key = lambda x:x[1][0], reverse = True)\n",
    "        how_many = len(sort_by_count)\n",
    "        if self.max_len > how_many:\n",
    "            how_many = how_many\n",
    "        elif self.max_len <= how_many:\n",
    "            how_many = self.max_len\n",
    "            \n",
    "        count = 0\n",
    "        index_of_words = {}\n",
    "        self.idfs = []\n",
    "        for key, value in sort_by_count:\n",
    "            if count >= how_many:\n",
    "                break\n",
    "            if self.stop_word == True:\n",
    "                if key not in stopwords.words(\"english\"):\n",
    "                    index_of_words[key] = count\n",
    "                    if self.tfidf == True:\n",
    "                        self.idfs.append(value[2])\n",
    "                    count += 1\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                index_of_words[key] = count\n",
    "                if self.tfidf == True:\n",
    "                    self.idfs.append(value[2])\n",
    "                count += 1\n",
    "\n",
    "        self.max_len = how_many\n",
    "        self.index_of_words = index_of_words\n",
    "                \n",
    "        \n",
    "    def tfidfVect(self, word_count):\n",
    "        nD = len(self.texts)\n",
    "        for key, value in sorted(word_count.items(), key = lambda x:x[1][0], reverse = True):\n",
    "            idf = math.log(nD/(1+value[1]))\n",
    "            word_count[key].append(idf)\n",
    "            word_count[key][0] = value[0] * idf\n",
    "        \n",
    "        self.make_vector(word_count)\n",
    "        \n",
    "    def countVect(self, word_count):\n",
    "        self.make_vector(word_count)\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        self.texts = texts\n",
    "        word_count = {}\n",
    "            \n",
    "        #word_count = {단어: [전체단어개수,단어가 나온 문장개수]}\n",
    "        for text in texts:\n",
    "            word_in_text = []\n",
    "            ts = text.split()\n",
    "            if self.ngram != False:\n",
    "                ts += self.n_gram(text, 2)\n",
    "            for word in ts:\n",
    "                if word not in word_count.keys():\n",
    "                    word_count[word] = [1, 1]\n",
    "                    word_in_text.append(word)\n",
    "                else:\n",
    "                    word_count[word][0] += 1\n",
    "                    word_in_text.append(word)\n",
    "            \n",
    "            for i in set(word_in_text):\n",
    "                word_count[i][1] += 1\n",
    "        \n",
    "        chaier = []\n",
    "        for key, value in word_count.items():\n",
    "            if value[0] > self.max_df and value[0] < self.min_df:\n",
    "                chaier.append(key)\n",
    "        for i in chaier:\n",
    "            del word_count[i]\n",
    "        \n",
    "        if self.tfidf:\n",
    "            self.tfidfVect(word_count)\n",
    "        \n",
    "        else:\n",
    "            self.countVect(word_count)\n",
    "        \n",
    "    def transform(self, texts):\n",
    "        ret_list = [[0 for i in range(self.max_len)] for i in range(len(texts))]\n",
    "        for i in range(len(texts)):\n",
    "            text = texts[i]\n",
    "            ts = text.split()\n",
    "            if self.ngram != False:\n",
    "                text = self.n_gram(text, 2)\n",
    "                ts = text\n",
    "            for word in ts:\n",
    "#                 print(word)\n",
    "                if word in self.index_of_words.keys():\n",
    "#                     print(self.index_of_words[word])\n",
    "                    ret_list[i][self.index_of_words[word]] += 1\n",
    "        \n",
    "        ret_list = np.array(ret_list)\n",
    "        idfs = np.array(self.idfs)\n",
    "        print(ret_list.shape, idfs.shape)\n",
    "        if self.tfidf == True:\n",
    "            ret_list = ret_list * idfs\n",
    "        \n",
    "        return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "\n",
    "    def fit_(self, X, y_):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "#         y_ = np.where(np.array(y) == 0, -1, 1)\n",
    "        # Problem 1: label을 1 또는 -1로 변환해주세요 (decision boundary)\n",
    "        \n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx]*(np.dot(self.w, np.array(x_i)) - self.b) >=1\n",
    "                # Problem 2: 위의 수식을 참고하여 코드를 완성해주세요\n",
    "                \n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(y_[idx], np.array(x_i)))\n",
    "                     # Problem 3a: 위의 수식을 참고하여 코드를 완성해주세요\n",
    "                    self.b -= self.lr * (y_[idx])\n",
    "                    # Problem 3b: 위의 수식을 참고하여 코드를 완성해주세요\n",
    "    \n",
    "    \n",
    "    def fit(self, x_tempo, y_tempo):\n",
    "        self.sortByindex =[None for i in range(len(set(y_tempo)))]\n",
    "        self.x_tempo = x_tempo\n",
    "        self.y_tempo = y_tempo\n",
    "        print(x_tempo.shape[0], y_tempo.shape[0])\n",
    "        for value in range(len(set(y_tempo))):\n",
    "            y_temp = []\n",
    "            for i in range(len(y_tempo)):\n",
    "                if y_tempo[i] == value:\n",
    "                    y_temp.append(1)\n",
    "                else:\n",
    "                    y_temp.append(-1)\n",
    "                    \n",
    "#             print(y_temp)\n",
    "                    \n",
    "            self.sortByindex[value] = y_temp\n",
    "                \n",
    "    def predict__(self, X):\n",
    "        approx = np.dot(X, self.w) + self.b\n",
    "        return approx\n",
    "#         return approx  \n",
    "        \n",
    "    def predict_(self, X):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return approx\n",
    "#         return approx\n",
    "\n",
    "    def predict(self, X):\n",
    "#         self.fin = [[-1 for i in range(len(set(self.y_tempo)))] for i in range(X.shape[0])]\n",
    "        self.fin = [0 for i in range(X.shape[0])]\n",
    "#         print(self.fin)\n",
    "#         print(X.shape)\n",
    "        real_fin = []\n",
    "        for value in range(len(set(self.y_tempo))):\n",
    "            x_temp = self.x_tempo\n",
    "            y_temp = np.array(self.sortByindex[value])\n",
    "#             print(x_temp, y_temp)\n",
    "            self.fit_(x_temp, y_temp)\n",
    "            prediction = clf.predict__(X)\n",
    "            real_fin.append(list(prediction))\n",
    "#             print(prediction)\n",
    "#             for i in range(prediction.shape[0]):\n",
    "#                 if int(prediction[i]) == 1:\n",
    "#                     self.fin[i]= value\n",
    "#                 self.fin[i][value] = prediction\n",
    "        real = []\n",
    "        \n",
    "        for text in range(X.shape[0]):\n",
    "            maxer = -float(\"inf\")\n",
    "            maxer_idx = -1\n",
    "            for label in range(len(real_fin)):\n",
    "#                 print(label)\n",
    "                if maxer < real_fin[label][text]:\n",
    "                    maxer_idx = label\n",
    "                    maxer = real_fin[label][text]\n",
    "            real.append(maxer_idx)\n",
    "        return np.array(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_spliter(X, y, test_rate = 0.858):\n",
    "    standard = int(len(X)* test_rate)\n",
    "    return (X[:standard], X[standard:]), (y[:standard], y[standard:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x_train,test_x_train), (train_y_train, test_y_train) = train_test_spliter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1000) (1000,)\n",
      "(50, 1000) (1000,)\n"
     ]
    }
   ],
   "source": [
    "cv = Vectorizer(max_len = 1500, tfidf = True, stop_word = True, max_df = 500,\n",
    "                 min_df = 10, bigram = True)\n",
    "cv.fit(x_train)\n",
    "# print(cv.index_of_words)\n",
    "# print(x_train)\n",
    "term_docs_train = cv.transform(train_x_train)\n",
    "term_docs_test = cv.transform(test_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 300\n"
     ]
    }
   ],
   "source": [
    "clf = SVM(learning_rate=0.01, lambda_param=0.001, n_iters=1000)\n",
    "clf.fit(term_docs_train, train_y_train)\n",
    "y_pred = clf.predict(term_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(a, b):\n",
    "    b = list(b)\n",
    "    a = list(a)\n",
    "    count = 0\n",
    "    for i in range(len(b)):\n",
    "        if a[i] == b[i]:\n",
    "            count+= 1\n",
    "    return count / len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_function(y_pred, test_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 1150) (1150,)\n",
      "(150, 1150) (1150,)\n"
     ]
    }
   ],
   "source": [
    "cv = Vectorizer(1150, tfidf = True, stop_word = False, max_df=500, min_df = 5, bigram = True)\n",
    "cv.fit(x_train)\n",
    "term_docs_train = cv.transform(x_train)\n",
    "test_term_docs = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 1150)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_term_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 350\n"
     ]
    }
   ],
   "source": [
    "clf = SVM(learning_rate=0.001, lambda_param=0.001, n_iters=1000)\n",
    "clf.fit(term_docs_train, y_train)\n",
    "test_pred = clf.predict(test_term_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_pred).to_csv(\"2018310412.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
